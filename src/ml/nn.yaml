# nn1_stable: Baseline configuration (good general purpose)
# nn2_deep: Deeper architecture to learn complex interactions
# nn3_fast: Wide, shallow model for faster convergence and lower training cost
nn1_stable:
  hidden_layer_sizes: [256, 128]
  activation: relu
  solver: adam
  learning_rate_init: 0.001
  batch_size: 512
  alpha: 0.0005
  early_stopping: true
  validation_fraction: 0.1
  max_iter: 1000
  random_state: 42

nn2_deep:
  hidden_layer_sizes: [512, 256, 128]
  activation: relu
  solver: adam
  learning_rate_init: 0.0005
  batch_size: 1024
  alpha: 0.001
  early_stopping: true
  validation_fraction: 0.15
  max_iter: 1500
  random_state: 42

nn3_fast:
  hidden_layer_sizes: [512]
  activation: tanh
  solver: adam
  learning_rate_init: 0.002
  batch_size: 2048
  alpha: 0.0001
  early_stopping: true
  validation_fraction: 0.1
  max_iter: 500
  random_state: 42